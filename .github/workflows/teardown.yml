run-name: Teardown

on:
  workflow_call:
    inputs:
      environment_id:
        required: true
        type: string
      environment_suffix:
        required: true
        type: string
      infra_folder_path:
        required: false
        type: string
        default: infra

jobs:
  teardown:
    runs-on: ubuntu-latest
    permissions:
      id-token: write
      contents: read
    steps:
      - name: Checkout
        id: checkout
        uses: actions/checkout@v4

      - name: Set up Python 3.10
        id: python-setup
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Extract Deployment Variables to Output
        id: extract-deployment-envs
        env:
          REVIEW_TEARDOWN_TOKEN: ${{ secrets.REVIEW_TEARDOWN_TOKEN }}
        run: |
          set -o allexport

          if [ -f "${{ inputs.infra_folder_path }}/environments/common.env" ]; then
            source ${{ inputs.infra_folder_path }}/environments/common.env
          else
            echo "No common.env file has been found. Disregarding it."
          fi

          source ${{ inputs.infra_folder_path }}/environments/${{ inputs.environment_id }}.env

          if [[ -z $ProductName || -z $AWS_REGION || -z $AWS_ACCOUNT_ID || -z $AWS_ACCOUNT_NAME || -z $AWS_CD_ROLE || -z $AWS_CF_ROLE ]]; then
            echo -e "All necessary parameters must be defined, which are:\n- ProductName\n- AWS_REGION\n- AWS_ACCOUNT_ID\n- AWS_ACCOUNT_NAME\n- AWS_CD_ROLE\n- AWS_CF_ROLE"
            exit 1
          else
            echo "All necessary parameters are defined"
          fi

          # shellcheck disable=SC2129
          echo "ProductName=$ProductName" >> "$GITHUB_OUTPUT"
          echo "AWS_REGION=$AWS_REGION" >> "$GITHUB_OUTPUT"
          echo "AWS_ACCOUNT_ID=$AWS_ACCOUNT_ID" >> "$GITHUB_OUTPUT"
          echo "AWS_ACCOUNT_NAME=$AWS_ACCOUNT_NAME" >> "$GITHUB_OUTPUT"
          echo "AWS_CD_ROLE=$AWS_CD_ROLE" >> "$GITHUB_OUTPUT"

      - name: Construct Stack Name
        id: construct-stackname
        run: |
          echo "STACK_NAME=${{ steps.extract-deployment-envs.outputs.ProductName }}-${{ inputs.environment_suffix }}" >> "$GITHUB_OUTPUT"

      - name: Configure AWS Credentials
        id: configure-aws-creds
        uses: aws-actions/configure-aws-credentials@v4.0.1
        with:
          role-to-assume: ${{ steps.extract-deployment-envs.outputs.AWS_CD_ROLE }}
          aws-region: ${{ steps.extract-deployment-envs.outputs.AWS_REGION }}

      - name: Check if Stack already exists
        id: check-stack-exists
        run: |
          if aws cloudformation describe-stacks --stack-name ${{ steps.construct-stackname.outputs.STACK_NAME }}; then
            echo "superstack_exists=1" >> "$GITHUB_OUTPUT"
          else
            echo "superstack_exists=0" >> "$GITHUB_OUTPUT"
          fi

      - name: Install Python Dependencies
        if: steps.check-stack-exists.outputs.superstack_exists == 1
        id: install-python-dependencies
        run: pip install boto3

      - name: Teardown Stack and Buckets
        if: steps.check-stack-exists.outputs.superstack_exists == 1
        id: teardown-stack-and-buckets
        shell: python
        run: |
          import sys
          import boto3
          import botocore.exceptions as boto3exceptions

          cfn = boto3.client("cloudformation")
          s3 = boto3.resource('s3')

          STACK = "${{ steps.construct-stackname.outputs.STACK_NAME }}"

          s3_bucket_list = []

          def get_s3_bucket_names(stack_name):
            response = cfn.list_stack_resources(StackName=stack_name)
            s3_buckets = [resource for resource in response['StackResourceSummaries'] if resource['ResourceType'] == 'AWS::S3::Bucket']
            stack_bucket_list = [bucket['PhysicalResourceId'] for bucket in s3_buckets]
            s3_bucket_list.extend(stack_bucket_list)
            nested_stacks = [resource for resource in response['StackResourceSummaries'] if resource['ResourceType'] == 'AWS::CloudFormation::Stack']
            for nested_stack in nested_stacks:
              nested_stack_name = nested_stack['PhysicalResourceId']
              get_s3_bucket_names(nested_stack_name)

          try:
            print ("Check if stack exists")
            stack_response = cfn.describe_stacks(StackName=STACK)

            print ("Get bucket names before stack deletion")
            get_s3_bucket_names( STACK )

            try:
              stack_deletion_response = cfn.delete_stack(StackName=STACK)
              print(f"Deleting stack {STACK}: {stack_deletion_response}")

              print("Deleting stack's buckets")
              for bucket_name in s3_bucket_list:
                bucket = s3.Bucket(bucket_name)
                print(bucket.name)
                object_versions_delete_response = bucket.object_versions.delete()
                bucket_delete_response = bucket.delete()
                print(object_versions_delete_response)
                print(bucket_delete_response)
            except boto3exceptions.ClientError as exc:
              print(str(exc))
              sys.exit(1)
          except boto3exceptions.ClientError as exc:
            print(str(exc))
            sys.exit(1)
