on:
  workflow_call:
    inputs:
      stack_name:
        required: true
        type: string
      environment_id:
        required: true
        type: string
      infra_folder_path:
        required: false
        type: string
        default: infra
      airflow_dags_folder:
        required: false
        type: string
        default: airflow_dags
      airflow_dags_artifact:
        required: false
        type: string
        description: "Name of the artifact containing generated Airflow DAGs"
        default: ""

jobs:
  sync-dags:
    name: Sync dags to Airflow
    runs-on: ubuntu-22.04
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download generated dags
        if: ${{ inputs.airflow_dags_artifact != '' }}
        uses: actions/download-artifact@v4
        with:
          name: ${{ inputs.airflow_dags_artifact }}
          path: ${{ inputs.airflow_dags_folder }}

      - name: Set deployment env
        id: set-environment
        run: |
          # shellcheck disable=SC2016
          cat ${{ inputs.infra_folder_path }}/environments/${{ inputs.environment_id }}.env >> "$GITHUB_ENV"

      - name: Configure AWS Credentials
        id: configure-aws-creds
        uses: aws-actions/configure-aws-credentials@v4.0.2
        with:
          role-to-assume: ${{ env.AWS_CD_ROLE }}
          aws-region: ${{ env.AWS_REGION }}
          mask-aws-account-id: false

      - name: Copy dags to S3 bucket
        id: s3-copy
        shell: bash
        run: |
          echo "Retrieving dags S3 bucket from CloudFormation stack"
          # shellcheck disable=SC2016
          DAGS_BUCKET=$(aws cloudformation describe-stacks \
            --stack-name "${{ inputs.stack_name }}" \
            --query 'Stacks[0].Outputs[?OutputKey==`AirflowDagsBucket`].OutputValue' \
            --output text)
          echo "Copy dags folder to S3"
          echo "DAGS_BUCKET=$DAGS_BUCKET" >> "$GITHUB_OUTPUT"
          aws s3 sync "${{ inputs.airflow_dags_folder }}/" "s3://$DAGS_BUCKET/" --delete

      - name: Copy dags from S3 to EFS
        id: data-sync
        shell: bash
        run: |
          echo "Run data sync to copy dags from S3 to EFS"
          # shellcheck disable=SC2016
          DATASYNC_TASK_ARN=$(aws cloudformation describe-stacks \
            --stack-name "${{ inputs.stack_name }}" \
            --query 'Stacks[0].Outputs[?OutputKey==`DataSyncTaskArn`].OutputValue' \
            --output text)
          TASK_EXEC_ARN=$(aws datasync start-task-execution --task-arn "$DATASYNC_TASK_ARN" --query TaskExecutionArn --output text)
          while true; do
            STATUS=$(aws datasync describe-task-execution --task-execution-arn "$TASK_EXEC_ARN" --query Status --output text)
            echo "Current status: $STATUS"
            if [[ "$STATUS" == "SUCCESS" || "$STATUS" == "ERROR" || "$STATUS" == "STOPPED" ]]; then
              break
            fi
            sleep 20
          done
