on:
  workflow_call:
    inputs:
      stack_name:
        required: true
        type: string
      environment_id:
        required: true
        type: string
      infra_folder_path:
        required: false
        type: string
        default: infra-airflow/        
      role_to_assume:
        required: true
        type: string
      airflow_dags_folder:
        required: false
        type: string
        default: airflow_dags
jobs:
  sync-dags:
    name: Sync dags to Airflow
    runs-on: ubuntu-22.04
    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Set deployment env
        id: set-environment
        run: |
          # shellcheck disable=SC2016
          cat ${{ inputs.infra_folder_path }}/environments/${{ inputs.environment_id }}.env >> "$GITHUB_ENV"
          
          if [ ${{ github.event_name }} == 'pull_request' ]; then
            echo "This is a review environment"
            cat ${{ needs.set-environment-variables.outputs.infra_folder_path }}environments/review.env >> "$GITHUB_ENV"
          else
            echo "This is a production environment"
            cat ${{ needs.set-environment-variables.outputs.infra_folder_path }}environments/production.env >> "$GITHUB_ENV"
          fi

      - name: Configure AWS Credentials
        id: configure-aws-creds
        uses: aws-actions/configure-aws-credentials@v4.0.1
        with:
          role-to-assume: ${{ inputs.role_to_assume }}
          aws-region: ${{ env.AWS_REGION }}
          mask-aws-account-id: false

      - name: Copy dags to S3 bucket
        id: s3-copy
        shell: bash
        run: |
          echo "Retrieving dags S3 bucket from CloudFormation stack"
          # shellcheck disable=SC2016
          DAGS_BUCKET=$(aws cloudformation describe-stacks \
            --stack-name "${{ inputs.stack_name }}" \
            --query 'Stacks[0].Outputs[?OutputKey==`AirflowDagsBucket`].OutputValue' \
            --output text)
          echo "Copy dags folder to S3"
          echo "DAGS_BUCKET=$DAGS_BUCKET" >> "$GITHUB_OUTPUT"
          aws s3 sync "${{ inputs.airflow_dags_folder }}/" "s3://$DAGS_BUCKET/"

      - name: Copy dags from S3 to EFS
        id: data-sync
        shell: bash
        run: |
          echo "Run data sync to copy dags from S3 to EFS"
          # shellcheck disable=SC2016
          DATASYNC_TASK_ARN=$(aws cloudformation describe-stacks \
            --stack-name "${{ inputs.stack_name }}" \
            --query 'Stacks[0].Outputs[?OutputKey==`DataSyncTaskArn`].OutputValue' \
            --output text)
          aws datasync start-task-execution --task-arn "$DATASYNC_TASK_ARN"
