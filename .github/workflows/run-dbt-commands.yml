name: Submit dbt Commands to Orfium's Centralized DBT Service

on:
  workflow_call:
    inputs:
      environment:
        description: 'DBT Service Environment (production or integration)'
        required: false
        default: integration
        type: string
      team:
        description: 'Team / Data Product name (lowercase)'
        required: true
        type: string
      git_ref:
        description: 'The git ref (branch name, commit SHA or tag) of your repository'
        required: true
        type: string
      steps:
        description: 'The list of command steps for dbt to execute. Note that the "dbt" command should be omitted.'
        required: true
        type: string
      snowflake_conn_id:
        description: 'The Snowflake connection that defines the configs of the custom profile to be used (defaults to "snowflake_conn_id_{team}")'
        required: false
        type: string
      wait_for_completion:
        description: 'If true, waits for dbt execution to complete.'
        required: false
        type: boolean
        default: true
      timeout:
        description: 'Timeout in seconds (max 3600)'
        required: false
        type: number
        default: 3600
      check_interval:
        description: 'Polling interval in seconds'
        required: false
        type: number
        default: 30
    secrets:
      DBT_SERVICE_AIRFLOW_USERNAME:
        required: true
      DBT_SERVICE_AIRFLOW_PASSWORD:
        required: true

jobs:
  submit-dbt-commands:
    runs-on: ubuntu-latest
    steps:
      - name: Validate environment input
        run: |
          if [[ "${{ inputs.environment }}" != "production" && "${{ inputs.environment }}" != "integration" ]]; then
            echo "Invalid environment! Must be either 'production' or 'integration'. Got '${{ inputs.environment }}'"
            exit 1
          fi

      - name: Set environment variables
        id: set-env
        run: |
          TEAM="${{ inputs.team }}"
          TARGET_DAG_ID="dbt_runner_${TEAM}"
          echo "TARGET_DAG_ID=${TARGET_DAG_ID}" >> "$GITHUB_ENV"

          # Set Airflow API endpoints based on env input
          if [ "${{ inputs.environment }}" == "production" ]; then
            echo "AIRFLOW_API_TRIGGER_DAG_RUN_ENDPOINT=https://data-platform-airflow.data.orfium.com/airflow/api/v1/dags/${TARGET_DAG_ID}/dagRuns" >> "$GITHUB_ENV"
            echo "AIRFLOW_WEBSERVER_BASE_URL=https://data-platform-airflow.data.orfium.com/airflow/dags/${TARGET_DAG_ID}/grid" >> "$GITHUB_ENV"
          else
            echo "AIRFLOW_API_TRIGGER_DAG_RUN_ENDPOINT=https://data-platform-airflow.dev.data.orfium.xyz/airflow/api/v1/dags/${TARGET_DAG_ID}/dagRuns" >> "$GITHUB_ENV"
            echo "AIRFLOW_WEBSERVER_BASE_URL=https://data-platform-airflow.dev.data.orfium.xyz/airflow/dags/${TARGET_DAG_ID}/grid" >> "$GITHUB_ENV"
          fi

          # Set snowflake_conn_id to default if not provided
          if [ -z "${{ inputs.snowflake_conn_id }}" ]; then
            echo "SNOWFLAKE_CONN_ID=snowflake_conn_id_${TEAM}" >> "$GITHUB_ENV"
          else
            echo "SNOWFLAKE_CONN_ID=${{ inputs.snowflake_conn_id }}" >> "$GITHUB_ENV"
          fi

          # Set headers
          echo 'DBT_ORFIUM_HEADERS={"Content-Type": "application/json"}' >> "$GITHUB_ENV"

          # Cap timeout at 3600 if it's larger
          TIMEOUT="${{ inputs.timeout }}"
          if [ "$TIMEOUT" -gt 3600 ]; then
            echo "Timeout value ${{ inputs.timeout }} exceeds maximum allowed (3600). Capping at 3600 seconds."
            TIMEOUT=3600
          fi
          echo "TIMEOUT=$TIMEOUT" >> "$GITHUB_ENV"

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install requests

      - name: Trigger Airflow DAG Run
        id: trigger-dag
        env:
          GIT_REF: ${{ inputs.git_ref }}
          DBT_STEPS: ${{ inputs.steps }}
          DBT_SERVICE_AIRFLOW_USERNAME: ${{ secrets.DBT_SERVICE_AIRFLOW_USERNAME }}
          DBT_SERVICE_AIRFLOW_PASSWORD: ${{ secrets.DBT_SERVICE_AIRFLOW_PASSWORD }}
        shell: python
        run: |
          import json
          import requests
          import os
          import sys

          from requests.adapters import HTTPAdapter, Retry
          from requests.exceptions import RequestException

          AIRFLOW_API_TRIGGER_DAG_RUN_ENDPOINT = os.environ.get('AIRFLOW_API_TRIGGER_DAG_RUN_ENDPOINT')
          GIT_REF = os.environ.get('GIT_REF')
          DBT_STEPS = os.environ.get('DBT_STEPS')
          SNOWFLAKE_CONN_ID = os.environ.get('SNOWFLAKE_CONN_ID')
          DBT_ORFIUM_HEADERS_DICT = json.loads(os.environ.get('DBT_ORFIUM_HEADERS'))
          DBT_SERVICE_AIRFLOW_USERNAME = os.environ.get('DBT_SERVICE_AIRFLOW_USERNAME')
          DBT_SERVICE_AIRFLOW_PASSWORD = os.environ.get('DBT_SERVICE_AIRFLOW_PASSWORD')

          auth = (DBT_SERVICE_AIRFLOW_USERNAME, DBT_SERVICE_AIRFLOW_PASSWORD)


          def post_with_retry(url, auth=None, data=None, headers=None, max_retries=3, timeout=5):
            """
            Send a POST request with automatic retries (with exponential backoff) for status codes in `{status_forcelist}`.

            See below for a list of the status codes that trigger an automatic retry
            https://www.restapitutorial.com/advanced/responses/retries
            """

            session = requests.Session()

            retries = Retry(
              total=max_retries,
              backoff_factor=1,  # exponential backoff: 0, 2, 4
              status_forcelist=[408, 429, 500, 502, 503, 504],
              allowed_methods={"POST"},
              respect_retry_after_header=True,  # will respect the Retry-After seconds in the headers in case of 429
              raise_on_status=True,  # will raise a MaxRetryError if retries have been exhausted and the status falls in the `status_forcelist`
              raise_on_redirect=True
            )

            adapter = HTTPAdapter(max_retries=retries)
            session.mount("https://", adapter)

            try:
              response = session.post(url, auth=auth, data=data, headers=headers, timeout=timeout)
              if response.status_code == 200:
                return response
              else:  # the LAST response status is neither 200 nor belongs to `status_forcelist`
                res_dict = response.json()  # safe to decode because it's an Airflow API response
                print(f"Status: {res_dict['status']}: {res_dict['title']} - {res_dict['detail']}")
                sys.exit(1)
            except RequestException as e:  # urllib3 MaxRetryError is wrapped in a subclass of RequestException
              print(f"Exception type: {type(e).__name__}")
              print(f"Exception message: {e}")
              sys.exit(1)


          data = {
            "conf": {
              "git_sha": GIT_REF,  # TODO: update when DBT Service auto-generated DAGs adopt the new naming
              "dbt_steps": DBT_STEPS,
              "snowflake_conn_id": SNOWFLAKE_CONN_ID
            }
          }

          response = post_with_retry(url=AIRFLOW_API_TRIGGER_DAG_RUN_ENDPOINT, auth=auth, headers=DBT_ORFIUM_HEADERS_DICT, data=json.dumps(data))

          dag_run_state = response.json()["state"]
          if dag_run_state != "failed":
            DAG_RUN_ID = response.json()["dag_run_id"]
            print(f"DBT Service DAG_RUN_ID: {DAG_RUN_ID}")

            with open(os.environ.get('GITHUB_ENV'), "a") as env_file:
              env_file.write(f"DAG_RUN_ID={DAG_RUN_ID}")
          else:
            print(f"dbt run ended prematurely with error status: {dag_run_state}")
            sys.exit(1)

      # Commented out until a solution to avoid masking is found
      #
      # - name: Output DBT Service Task Logs URL
      #   shell: python
      #   env:
      #     DAG_RUN_ID: ${{ env.DAG_RUN_ID }}
      #     AIRFLOW_WEBSERVER_BASE_URL: ${{ env.AIRFLOW_WEBSERVER_BASE_URL }}
      #   run: |
      #     import os
      #     from urllib.parse import quote
          
      #     DAG_RUN_ID = os.environ.get('DAG_RUN_ID')
      #     AIRFLOW_WEBSERVER_BASE_URL = os.environ.get('AIRFLOW_WEBSERVER_BASE_URL')
      #     DBT_SERVICE_TASK_LOGS_URL = f"{AIRFLOW_WEBSERVER_BASE_URL}?dag_run_id={quote(DAG_RUN_ID)}&task_id=dbt_task&tab=logs"

      #     print(f"DBT Service Task Logs URL:\n{DBT_SERVICE_TASK_LOGS_URL}")

      - name: Retrieve Logs
        if: ${{ inputs.wait_for_completion }} == true
        env:
          CHECK_INTERVAL: ${{ inputs.check_interval }}
          DBT_SERVICE_AIRFLOW_USERNAME: ${{ secrets.DBT_SERVICE_AIRFLOW_USERNAME }}
          DBT_SERVICE_AIRFLOW_PASSWORD: ${{ secrets.DBT_SERVICE_AIRFLOW_PASSWORD }}
        shell: python
        run: |
          import json
          import os
          import requests
          import sys
          import time

          from requests.adapters import HTTPAdapter, Retry
          from requests.exceptions import RequestException

          # get env variables
          DAG_RUN_ID = os.environ.get('DAG_RUN_ID')
          AIRFLOW_API_TRIGGER_DAG_RUN_ENDPOINT = os.environ.get('AIRFLOW_API_TRIGGER_DAG_RUN_ENDPOINT')
          DBT_ORFIUM_HEADERS_DICT = json.loads(os.environ.get('DBT_ORFIUM_HEADERS'))
          TIMEOUT = float(os.environ.get('TIMEOUT'))  # using the sanitised value instead of the direct input (which might exceed 3600)
          CHECK_INTERVAL = float(os.environ.get('CHECK_INTERVAL'))
          DBT_SERVICE_AIRFLOW_USERNAME = os.environ.get('DBT_SERVICE_AIRFLOW_USERNAME')
          DBT_SERVICE_AIRFLOW_PASSWORD = os.environ.get('DBT_SERVICE_AIRFLOW_PASSWORD')

          # Airflow Statuses
          TERMINAL_STATUSES = ("failed", "success")
          ERRONEOUS_STATUSES = ("failed")
          
          # construct API polling endpoint
          AIRFLOW_API_GET_DAG_RUN_ENDPOINT = f"{AIRFLOW_API_TRIGGER_DAG_RUN_ENDPOINT}/{DAG_RUN_ID}"

          # construct API get logs endpoint
          AIRFLOW_API_GET_LOGS_ENDPOINT = f"{AIRFLOW_API_GET_DAG_RUN_ENDPOINT}/taskInstances/dbt_task/logs/1"

          auth = (DBT_SERVICE_AIRFLOW_USERNAME, DBT_SERVICE_AIRFLOW_PASSWORD)


          def get_with_retry(url, auth=None, params=None, headers=None, max_retries=3, timeout=5):
            """
            Send a GET request with automatic retries (with exponential backoff) for status codes in `{status_forcelist}`.

            See below for a list of the status codes that trigger an automatic retry
            https://www.restapitutorial.com/advanced/responses/retries
            """

            session = requests.Session()

            retries = Retry(
              total=max_retries,
              backoff_factor=1,  # exponential backoff: 0, 2, 4
              status_forcelist=[408, 429, 500, 502, 503, 504],
              allowed_methods={"GET"},
              respect_retry_after_header=True,  # will respect the Retry-After seconds in the headers in case of 429
              raise_on_status=True,  # will raise a MaxRetryError if retries have been exhausted and the status falls in the `status_forcelist`
              raise_on_redirect=True
            )

            adapter = HTTPAdapter(max_retries=retries)
            session.mount("https://", adapter)

            try:
              response = session.get(url, auth=auth, params=params, headers=headers, timeout=timeout)
              if response.status_code == 200:
                return response
              else:  # the LAST response status is neither 200 nor belongs to `status_forcelist`
                res_dict = response.json()  # safe to decode because it's an Airflow API response
                print(f"Status: {res_dict['status']}: {res_dict['title']} - {res_dict['detail']}")
                sys.exit(1)
            except RequestException as e:  # urllib3 MaxRetryError is wrapped in a subclass of RequestException
              print(f"Exception type: {type(e).__name__}")
              print(f"Exception message: {e}")
              sys.exit(1)


          def retrieve_logs():
            logs_headers = {
              **DBT_ORFIUM_HEADERS_DICT,
              **{
                "Accept": "text/plain"
              },  # explicitly specifying to respond with text format.
            }

            # The Airflow API TaskInstance#get_logs supports this query parameter
            # to specify whether to retrieve only the first log fragment or the full content.
            params = {"full_content": True}

            response = get_with_retry(AIRFLOW_API_GET_LOGS_ENDPOINT, auth=auth, headers=logs_headers, params=params)
            return response.text


          def get_dag_run_status():
            response = get_with_retry(AIRFLOW_API_GET_DAG_RUN_ENDPOINT, auth=auth, headers=DBT_ORFIUM_HEADERS_DICT)
            status = response.json().get("state")
            return status


          end_time = time.time() + TIMEOUT

          status = get_dag_run_status()

          while not status in TERMINAL_STATUSES:
            if end_time < time.time():
              print(f"dbt run {DAG_RUN_ID} has not reached a terminal status after {end_time} seconds")
              sys.exit(1)

            time.sleep(CHECK_INTERVAL)
            status = get_dag_run_status()

            if status in ERRONEOUS_STATUSES:
              print(f"dbt run {DAG_RUN_ID} ended with error status:{status}")
              sys.exit(1)

          print(retrieve_logs())
